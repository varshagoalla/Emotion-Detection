{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"part1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M4piRnBLZJny","executionInfo":{"status":"ok","timestamp":1611124902750,"user_tz":-330,"elapsed":97735,"user":{"displayName":"Dipen Kumar","photoUrl":"https://lh5.googleusercontent.com/-2cjD9vSugmg/AAAAAAAAAAI/AAAAAAAAB0A/UYLAFNFu364/s64/photo.jpg","userId":"00395031505879820351"}},"outputId":"cf759616-e37d-4288-e737-2907b52ad99d"},"source":["import sys\n","import math\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from skimage.feature import hog\n","from skimage.filters import edges\n","from skimage.transform import resize\n","from skimage.filters import gabor_kernel\n","\n","file_train = open(\"/content/drive/MyDrive/Colab Notebooks/COL774/Assignment4/2018CS50098/train.csv\", \"r\")\n","data_train = torch.Tensor([[float(cell) for cell in row.split(\",\")] for row in file_train])\n","file_test = open(\"/content/drive/MyDrive/Colab Notebooks/COL774/Assignment4/2018CS50098/public_test.csv\", \"r\")\n","data_test = torch.Tensor([[float(cell) for cell in row.split(\",\")] for row in file_test])\n","\n","cmd = \"3\"\n","\n","if cmd == \"3\": # (20 points) Convolutional Neural Network\n","\n","    class CNN(nn.Module):\n","\n","        def __init__(self, n, batch_size, r):\n","            super(CNN, self).__init__()\n","            self.root_n = int(math.sqrt(n))\n","            self.conv1 = nn.Conv2d(1, 64, (3, 3), stride=3, padding=0)\n","            self.norm1 = nn.BatchNorm2d(64)\n","            self.pool1 = nn.MaxPool2d((2, 2), stride=2, padding=0)\n","            self.conv2 = nn.Conv2d(64, 128, (2, 2), stride=2, padding=0)\n","            self.norm2 = nn.BatchNorm2d(128)\n","            self.pool2 = nn.MaxPool2d((2, 2), stride=2, padding=0)\n","            self.fc1 = nn.Linear(128 * ((((((((self.root_n-3)//3+1)-2)//2+1)-2)//2+1)-2)//2+1)**2, 256)\n","            self.norm3 = nn.BatchNorm1d(256)\n","            self.fc2 = nn.Linear(256, r)\n","            self.batch_size = batch_size\n","            self.optimizer = torch.optim.SGD(self.parameters(), lr=0.01, momentum=0.9)\n","            self.loss_function = nn.CrossEntropyLoss()\n","\n","        def forward(self, X):\n","            X = X.view(-1, 1, self.root_n, self.root_n)\n","            X = F.relu(self.conv1(X))\n","            X = self.norm1(X)\n","            X = self.pool1(X)\n","            X = F.relu(self.conv2(X))\n","            X = self.norm2(X)\n","            X = self.pool2(X)\n","            X = X.view(-1, 128 * ((((((((self.root_n-3)//3+1)-2)//2+1)-2)//2+1)-2)//2+1)**2)\n","            X = F.relu(self.fc1(X))\n","            X = self.norm3(X)\n","            X = self.fc2(X)\n","            return X\n","\n","        def train(self, data):\n","            if torch.cuda.is_available():\n","                self = self.cuda()\n","                data = data.cuda()\n","            max_epoch = 75\n","            for epoch in range(max_epoch):\n","                cost = 0\n","                data = data[torch.randperm(data.shape[0]),:]\n","                for batch in range(math.ceil(data.shape[0]/self.batch_size)):\n","                    self.optimizer.zero_grad()\n","                    output = self(data[batch*self.batch_size:(batch+1)*self.batch_size,1:])\n","                    loss = self.loss_function(output, data[batch*self.batch_size:(batch+1)*self.batch_size,0].long())\n","                    loss.backward()\n","                    self.optimizer.step()\n","                    cost += float(loss.data) * data[batch*self.batch_size:(batch+1)*self.batch_size,:].shape[0]\n","                print(f\"Epochs = {epoch+1} and Loss = {round(cost/data.shape[0],6)}\")\n","            if torch.cuda.is_available():\n","                self = self.cpu()\n","                data = data.cpu()\n","\n","        def test(self, data):\n","            if torch.cuda.is_available():\n","                self = self.cuda()\n","                data = data.cuda()\n","            cost = 0\n","            correct = 0\n","            for batch in range (math.ceil(data.shape[0]/self.batch_size)):\n","                output = self(data[batch*self.batch_size:(batch+1)*self.batch_size,1:])\n","                loss = self.loss_function(output, data[batch*self.batch_size:(batch+1)*self.batch_size,0].long())\n","                cost += float(loss.data) * data[batch*self.batch_size:(batch+1)*self.batch_size,:].shape[0]\n","                prediction = F.softmax(output.data, dim=1).max(1)[1]\n","                correct += float(prediction.eq(data[batch*self.batch_size:(batch+1)*self.batch_size,0]).sum())\n","            print(f\"Accuracy = {round(100*correct/data.shape[0],2)}% and Loss = {round(cost/data.shape[0],6)}\")\n","            if torch.cuda.is_available():\n","                self = self.cpu()\n","                data = data.cpu()\n","\n","        def predict(self, data):\n","            if torch.cuda.is_available():\n","                self = self.cuda()\n","                data = data.cuda()\n","            y = []\n","            for batch in range (math.ceil(data.shape[0]/self.batch_size)):\n","                output = self(data[batch*self.batch_size:(batch+1)*self.batch_size,1:])\n","                prediction = F.softmax(output.data, dim=1).max(1)[1]\n","                y.extend(prediction)\n","            if torch.cuda.is_available():\n","                self = self.cpu()\n","                data = data.cpu()\n","            return y\n","\n","    cnn = CNN(data_train.shape[1]-1, 64, 7)\n","    cnn.train(data_train)\n","    cnn.test(data_train)\n","    cnn.test(data_test)\n","    np.savetxt(\"/content/drive/MyDrive/Colab Notebooks/COL774/Assignment4/2018CS50098/y\"+cmd+\"_nc.csv\", cnn.predict(data_test), fmt=\"%d\", delimiter=\"\\n\")\n","\n","if cmd == \"2\": # (10 points) Feature Engineering\n","\n","    count = 0\n","    filter = \"gabor\"\n","    def apply(filter, image):\n","        if filter == \"gabor\":\n","            accum = np.zeros_like(image)\n","            for kernel in kernels:\n","                filtered = edges.convolve(image, kernel, mode='wrap')\n","                np.maximum(accum, filtered, accum)\n","            image = accum\n","        elif filter == \"hog\":\n","            image = resize(image, (128, 64))\n","            image = hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), multichannel=False)[1]\n","            image = resize(image, (48, 48))\n","        global count\n","        count += 1\n","        print(f\"Images Filtered = {count}\")\n","        return image\n","\n","    kernels = []\n","    for theta in np.arange(0, np.pi, np.pi/4):\n","        for frequency in (0.05, 0.25):\n","            for sigma in (1, 3):\n","                kernel = np.real(gabor_kernel(frequency, theta=theta, sigma_x=sigma, sigma_y=sigma))\n","                kernels.append(kernel)\n","\n","    image_train = data_train[:,1:].view(-1, 48, 48).numpy()\n","    image_new_train = torch.Tensor([apply(filter, image) for image in image_train])\n","    data_train = torch.cat([data_train[:,0].unsqueeze(dim=1), image_new_train.view(-1, 2304)], dim=1).float()\n","\n","    image_test = data_test[:,1:].view(-1, 48, 48).numpy()\n","    image_new_test = torch.Tensor([apply(filter, image) for image in image_test])\n","    data_test = torch.cat([data_test[:,0].unsqueeze(dim=1), image_new_test.view(-1, 2304)], dim=1).float()\n","\n","if cmd == \"1\" or cmd == \"2\": # (10 points) Vanilla Neural Network\n","\n","    class NN_Model(nn.Module):\n","\n","        def __init__(self, n, batch_size, r):\n","            super(NN_Model, self).__init__()\n","            self.fc1 = nn.Linear(n,100)\n","            self.norm1 = nn.BatchNorm1d(100)\n","            self.fc2 = nn.Linear(100,r)\n","            self.batch_size = batch_size\n","            self.optimizer = torch.optim.SGD(self.parameters(), lr=0.01, momentum=0.9)\n","            self.loss_function = nn.CrossEntropyLoss()\n","\n","        def forward(self, X):\n","            X = F.relu(self.fc1(X))\n","            X = self.norm1(X)\n","            X = self.fc2(X)\n","            return X\n","\n","        def train(self, data):\n","            if torch.cuda.is_available():\n","                self = self.cuda()\n","                data = data.cuda()\n","            max_epoch = 100\n","            if cmd == \"2\":\n","                max_epoch = 500\n","            for epoch in range(max_epoch):\n","                cost = 0\n","                data = data[torch.randperm(data.shape[0]),:]\n","                for batch in range(math.ceil(data.shape[0]/self.batch_size)):\n","                    self.optimizer.zero_grad()\n","                    output = self(data[batch*self.batch_size:(batch+1)*self.batch_size,1:])\n","                    loss = self.loss_function(output, data[batch*self.batch_size:(batch+1)*self.batch_size,0].long())\n","                    loss.backward()\n","                    self.optimizer.step()\n","                    cost += float(loss.data) * data[batch*self.batch_size:(batch+1)*self.batch_size,:].shape[0]\n","                print(f\"Epochs = {epoch+1} and Loss = {round(cost/data.shape[0],6)}\")\n","            if torch.cuda.is_available():\n","                self = self.cpu()\n","                data = data.cpu()\n","\n","        def test(self, data):\n","            if torch.cuda.is_available():\n","                self = self.cuda()\n","                data = data.cuda()\n","            cost = 0\n","            correct = 0\n","            for batch in range (math.ceil(data.shape[0]/self.batch_size)):\n","                output = self(data[batch*self.batch_size:(batch+1)*self.batch_size,1:])\n","                loss = self.loss_function(output, data[batch*self.batch_size:(batch+1)*self.batch_size,0].long())\n","                cost += float(loss.data) * data[batch*self.batch_size:(batch+1)*self.batch_size,:].shape[0]\n","                prediction = F.softmax(output.data, dim=1).max(1)[1]\n","                correct += float(prediction.eq(data[batch*self.batch_size:(batch+1)*self.batch_size,0]).sum())\n","            print(f\"Accuracy = {round(100*correct/data.shape[0],2)}% and Loss = {round(cost/data.shape[0],6)}\")\n","            if torch.cuda.is_available():\n","                self = self.cpu()\n","                data = data.cpu()\n","\n","        def predict(self, data):\n","            if torch.cuda.is_available():\n","                self = self.cuda()\n","                data = data.cuda()\n","            y = []\n","            for batch in range (math.ceil(data.shape[0]/self.batch_size)):\n","                output = self(data[batch*self.batch_size:(batch+1)*self.batch_size,1:])\n","                prediction = F.softmax(output.data, dim=1).max(1)[1]\n","                y.extend(prediction)\n","            if torch.cuda.is_available():\n","                self = self.cpu()\n","                data = data.cpu()\n","            return y\n","\n","    nn_model = NN_Model(data_train.shape[1]-1, 256, 7)\n","    nn_model.train(data_train)\n","    nn_model.test(data_train)\n","    nn_model.test(data_test)\n","    np.savetxt(\"/content/drive/MyDrive/Colab Notebooks/COL774/Assignment4/2018CS50098/y\"+cmd+\"_nc.csv\", nn_model.predict(data_test), fmt=\"%d\", delimiter=\"\\n\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Epochs = 1 and Loss = 1.758917\n","Epochs = 2 and Loss = 1.595431\n","Epochs = 3 and Loss = 1.504843\n","Epochs = 4 and Loss = 1.437355\n","Epochs = 5 and Loss = 1.378704\n","Epochs = 6 and Loss = 1.34366\n","Epochs = 7 and Loss = 1.240201\n","Epochs = 8 and Loss = 1.173965\n","Epochs = 9 and Loss = 1.141736\n","Epochs = 10 and Loss = 1.058263\n","Epochs = 11 and Loss = 1.011823\n","Epochs = 12 and Loss = 1.050474\n","Epochs = 13 and Loss = 0.905678\n","Epochs = 14 and Loss = 0.809604\n","Epochs = 15 and Loss = 0.739953\n","Epochs = 16 and Loss = 0.69717\n","Epochs = 17 and Loss = 0.699411\n","Epochs = 18 and Loss = 0.601983\n","Epochs = 19 and Loss = 0.636423\n","Epochs = 20 and Loss = 0.487345\n","Epochs = 21 and Loss = 0.452175\n","Epochs = 22 and Loss = 0.397248\n","Epochs = 23 and Loss = 0.367044\n","Epochs = 24 and Loss = 0.338266\n","Epochs = 25 and Loss = 0.365085\n","Epochs = 26 and Loss = 0.283463\n","Epochs = 27 and Loss = 0.245188\n","Epochs = 28 and Loss = 0.285361\n","Epochs = 29 and Loss = 0.341004\n","Epochs = 30 and Loss = 0.227936\n","Epochs = 31 and Loss = 0.18346\n","Epochs = 32 and Loss = 0.158889\n","Epochs = 33 and Loss = 0.139099\n","Epochs = 34 and Loss = 0.142418\n","Epochs = 35 and Loss = 0.139212\n","Epochs = 36 and Loss = 0.119317\n","Epochs = 37 and Loss = 0.143305\n","Epochs = 38 and Loss = 0.123885\n","Epochs = 39 and Loss = 0.114877\n","Epochs = 40 and Loss = 0.100191\n","Epochs = 41 and Loss = 0.095441\n","Epochs = 42 and Loss = 0.09674\n","Epochs = 43 and Loss = 0.083678\n","Epochs = 44 and Loss = 0.078487\n","Epochs = 45 and Loss = 0.088471\n","Epochs = 46 and Loss = 0.096519\n","Epochs = 47 and Loss = 0.092907\n","Epochs = 48 and Loss = 0.088429\n","Epochs = 49 and Loss = 0.085816\n","Epochs = 50 and Loss = 0.082844\n","Epochs = 51 and Loss = 0.07235\n","Epochs = 52 and Loss = 0.080235\n","Epochs = 53 and Loss = 0.073939\n","Epochs = 54 and Loss = 0.153808\n","Epochs = 55 and Loss = 0.154785\n","Epochs = 56 and Loss = 0.093202\n","Epochs = 57 and Loss = 0.078923\n","Epochs = 58 and Loss = 0.069742\n","Epochs = 59 and Loss = 0.062149\n","Epochs = 60 and Loss = 0.055578\n","Epochs = 61 and Loss = 0.051695\n","Epochs = 62 and Loss = 0.045351\n","Epochs = 63 and Loss = 0.042702\n","Epochs = 64 and Loss = 0.039276\n","Epochs = 65 and Loss = 0.036819\n","Epochs = 66 and Loss = 0.036052\n","Epochs = 67 and Loss = 0.035335\n","Epochs = 68 and Loss = 0.035967\n","Epochs = 69 and Loss = 0.033593\n","Epochs = 70 and Loss = 0.028971\n","Epochs = 71 and Loss = 0.041612\n","Epochs = 72 and Loss = 0.034502\n","Epochs = 73 and Loss = 0.031387\n","Epochs = 74 and Loss = 0.029532\n","Epochs = 75 and Loss = 0.041905\n","Accuracy = 99.22% and Loss = 0.027731\n","Accuracy = 41.8% and Loss = 4.989532\n"],"name":"stdout"}]}]}