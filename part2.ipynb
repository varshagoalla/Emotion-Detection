{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"part2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v-031_6ZZzNw","executionInfo":{"status":"ok","timestamp":1611127907869,"user_tz":-330,"elapsed":4662066,"user":{"displayName":"Dipen Kumar","photoUrl":"https://lh5.googleusercontent.com/-2cjD9vSugmg/AAAAAAAAAAI/AAAAAAAAB0A/UYLAFNFu364/s64/photo.jpg","userId":"00395031505879820351"}},"outputId":"6cc0f7f2-2e5a-4a8a-d3f6-b854ff77dc61"},"source":["import sys\n","import math\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","file_train = open(\"/content/drive/MyDrive/Colab Notebooks/COL774/Assignment4/2018CS50098/train.csv\", \"r\")\n","data_train = torch.Tensor([[float(cell) for cell in row.split(\",\")] for row in file_train])\n","file_test1 = open(\"/content/drive/MyDrive/Colab Notebooks/COL774/Assignment4/2018CS50098/public_test.csv\", \"r\")\n","data_test1 = torch.Tensor([[float(cell) for cell in row.split(\",\")] for row in file_test1])\n","file_test2 = open(\"/content/drive/MyDrive/Colab Notebooks/COL774/Assignment4/2018CS50098/private.csv\", \"r\")\n","data_test2 = torch.Tensor([[float(cell) for cell in row.split(\",\")] for row in file_test2])\n","\n","class CNN(nn.Module):\n","\n","    def __init__(self, n, batch_size, r):\n","        super(CNN, self).__init__()\n","        self.root_n = int(math.sqrt(n))\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(1, 64, (3, 3), stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(64),\n","            nn.Conv2d(64, 64, (3, 3), stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(64),\n","            nn.Conv2d(64, 64, (3, 3), stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(64),\n","            nn.MaxPool2d((2, 2), stride=2, padding=0),\n","            nn.Dropout(p=0.5),\n","            nn.Conv2d(64, 128, (3, 3), stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(128),\n","            nn.Conv2d(128, 128, (3, 3), stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(128),\n","            nn.Conv2d(128, 128, (3, 3), stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(128),\n","            nn.MaxPool2d((2, 2), stride=2, padding=0),\n","            nn.Dropout(p=0.5),\n","            nn.Conv2d(128, 256, (3, 3), stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(256),\n","            nn.Conv2d(256, 256, (3, 3), stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(256),\n","            nn.Conv2d(256, 256, (3, 3), stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(256),\n","            nn.MaxPool2d((2, 2), stride=2, padding=0),\n","            nn.Dropout(p=0.5),\n","            nn.Conv2d(256, 512, (3, 3), stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(512),\n","            nn.Conv2d(512, 512, (3, 3), stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(512),\n","            nn.Conv2d(512, 512, (3, 3), stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(512),\n","            nn.MaxPool2d((2, 2), stride=2, padding=0),\n","            nn.Dropout(p=0.5)\n","        )\n","        self.linear = nn.Sequential(\n","            nn.Linear(512*3*3,512),\n","            nn.ReLU(),\n","            nn.Dropout(p=0.5),\n","            nn.Linear(512,256),\n","            nn.ReLU(),\n","            nn.Dropout(p=0.5),\n","            nn.Linear(256,128),\n","            nn.ReLU(),\n","            nn.Dropout(p=0.5),\n","            nn.Linear(128,64),\n","            nn.ReLU(),\n","            nn.Dropout(p=0.5),\n","            nn.Linear(64,r)\n","        )\n","        self.batch_size = batch_size\n","        self.optimizer = torch.optim.SGD(self.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-5)\n","        self.loss_function = nn.CrossEntropyLoss()\n","\n","    def forward(self, X):\n","        X = X.view(-1, 1, self.root_n, self.root_n)\n","        X = self.conv(X)\n","        X = X.view(-1, 3*3*512)\n","        X = self.linear(X)\n","        return X\n","\n","    def train(self, data):\n","        if torch.cuda.is_available():\n","            self = self.cuda()\n","            data = data.cuda()\n","        max_epoch = 200\n","        for epoch in range(max_epoch):\n","            cost = 0\n","            data = data[torch.randperm(data.shape[0]),:]\n","            for batch in range(math.ceil(data.shape[0]/self.batch_size)):\n","                self.optimizer.zero_grad()\n","                output = self(data[batch*self.batch_size:(batch+1)*self.batch_size,1:])\n","                loss = self.loss_function(output, data[batch*self.batch_size:(batch+1)*self.batch_size,0].long())\n","                loss.backward()\n","                self.optimizer.step()\n","                cost += float(loss.data) * data[batch*self.batch_size:(batch+1)*self.batch_size,:].shape[0]\n","            print(f\"Epochs = {epoch+1} and Loss = {round(cost/data.shape[0],6)}\")\n","        if torch.cuda.is_available():\n","            self = self.cpu()\n","            data = data.cpu()\n","\n","    def test(self, data):\n","        if torch.cuda.is_available():\n","            self = self.cuda()\n","            data = data.cuda()\n","        cost = 0\n","        correct = 0\n","        for batch in range (math.ceil(data.shape[0]/self.batch_size)):\n","            output = self(data[batch*self.batch_size:(batch+1)*self.batch_size,1:])\n","            loss = self.loss_function(output, data[batch*self.batch_size:(batch+1)*self.batch_size,0].long())\n","            cost += float(loss.data) * data[batch*self.batch_size:(batch+1)*self.batch_size,:].shape[0]\n","            prediction = F.softmax(output.data, dim=1).max(1)[1]\n","            correct += float(prediction.eq(data[batch*self.batch_size:(batch+1)*self.batch_size,0]).sum())\n","        print(f\"Accuracy = {round(100*correct/data.shape[0],2)}% and Loss = {round(cost/data.shape[0],6)}\")\n","        if torch.cuda.is_available():\n","            self = self.cpu()\n","            data = data.cpu()\n","\n","    def predict(self, data):\n","        if torch.cuda.is_available():\n","            self = self.cuda()\n","            data = data.cuda()\n","        y = []\n","        for batch in range (math.ceil(data.shape[0]/self.batch_size)):\n","            output = self(data[batch*self.batch_size:(batch+1)*self.batch_size,1:])\n","            prediction = F.softmax(output.data, dim=1).max(1)[1]\n","            y.extend(prediction)\n","        y = [f\"Id,Prediction\"] + [f\"{i+1},{y[i]}\" for i in range(len(y))]\n","        if torch.cuda.is_available():\n","            self = self.cpu()\n","            data = data.cpu()\n","        return y\n","\n","cnn = CNN(data_train.shape[1]-1, 64, 7)\n","cnn.train(data_train) \n","cnn.test(data_train)\n","cnn.test(data_test1)\n","np.savetxt(\"/content/drive/MyDrive/Colab Notebooks/COL774/Assignment4/2018CS50098/y_c.csv\", cnn.predict(data_test2), fmt=\"%s\", delimiter=\"\\n\")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Epochs = 1 and Loss = 1.825229\n","Epochs = 2 and Loss = 1.792486\n","Epochs = 3 and Loss = 1.75542\n","Epochs = 4 and Loss = 1.644001\n","Epochs = 5 and Loss = 1.535361\n","Epochs = 6 and Loss = 1.471291\n","Epochs = 7 and Loss = 1.419985\n","Epochs = 8 and Loss = 1.39299\n","Epochs = 9 and Loss = 1.368972\n","Epochs = 10 and Loss = 1.345035\n","Epochs = 11 and Loss = 1.314111\n","Epochs = 12 and Loss = 1.303246\n","Epochs = 13 and Loss = 1.271466\n","Epochs = 14 and Loss = 1.255286\n","Epochs = 15 and Loss = 1.235623\n","Epochs = 16 and Loss = 1.221437\n","Epochs = 17 and Loss = 1.204054\n","Epochs = 18 and Loss = 1.18809\n","Epochs = 19 and Loss = 1.181746\n","Epochs = 20 and Loss = 1.155309\n","Epochs = 21 and Loss = 1.138495\n","Epochs = 22 and Loss = 1.120703\n","Epochs = 23 and Loss = 1.121047\n","Epochs = 24 and Loss = 1.105348\n","Epochs = 25 and Loss = 1.088078\n","Epochs = 26 and Loss = 1.077777\n","Epochs = 27 and Loss = 1.060622\n","Epochs = 28 and Loss = 1.058017\n","Epochs = 29 and Loss = 1.039068\n","Epochs = 30 and Loss = 1.028075\n","Epochs = 31 and Loss = 1.004852\n","Epochs = 32 and Loss = 0.993474\n","Epochs = 33 and Loss = 0.985331\n","Epochs = 34 and Loss = 0.977825\n","Epochs = 35 and Loss = 0.963591\n","Epochs = 36 and Loss = 0.959397\n","Epochs = 37 and Loss = 0.946768\n","Epochs = 38 and Loss = 0.923894\n","Epochs = 39 and Loss = 0.924764\n","Epochs = 40 and Loss = 0.91128\n","Epochs = 41 and Loss = 0.894361\n","Epochs = 42 and Loss = 0.879946\n","Epochs = 43 and Loss = 0.875686\n","Epochs = 44 and Loss = 0.867474\n","Epochs = 45 and Loss = 0.850231\n","Epochs = 46 and Loss = 0.827178\n","Epochs = 47 and Loss = 0.829342\n","Epochs = 48 and Loss = 0.823859\n","Epochs = 49 and Loss = 0.79226\n","Epochs = 50 and Loss = 0.795707\n","Epochs = 51 and Loss = 0.782717\n","Epochs = 52 and Loss = 0.771238\n","Epochs = 53 and Loss = 0.755994\n","Epochs = 54 and Loss = 0.73866\n","Epochs = 55 and Loss = 0.719138\n","Epochs = 56 and Loss = 0.719758\n","Epochs = 57 and Loss = 0.701689\n","Epochs = 58 and Loss = 0.702676\n","Epochs = 59 and Loss = 0.67228\n","Epochs = 60 and Loss = 0.666738\n","Epochs = 61 and Loss = 0.645769\n","Epochs = 62 and Loss = 0.647092\n","Epochs = 63 and Loss = 0.630851\n","Epochs = 64 and Loss = 0.640821\n","Epochs = 65 and Loss = 0.620305\n","Epochs = 66 and Loss = 0.607662\n","Epochs = 67 and Loss = 0.589378\n","Epochs = 68 and Loss = 0.576153\n","Epochs = 69 and Loss = 0.557845\n","Epochs = 70 and Loss = 0.551032\n","Epochs = 71 and Loss = 0.552498\n","Epochs = 72 and Loss = 0.511619\n","Epochs = 73 and Loss = 0.500327\n","Epochs = 74 and Loss = 0.495271\n","Epochs = 75 and Loss = 0.491527\n","Epochs = 76 and Loss = 0.475839\n","Epochs = 77 and Loss = 0.469016\n","Epochs = 78 and Loss = 0.459443\n","Epochs = 79 and Loss = 0.450714\n","Epochs = 80 and Loss = 0.440073\n","Epochs = 81 and Loss = 0.447342\n","Epochs = 82 and Loss = 0.423874\n","Epochs = 83 and Loss = 0.433377\n","Epochs = 84 and Loss = 0.419125\n","Epochs = 85 and Loss = 0.396957\n","Epochs = 86 and Loss = 0.389269\n","Epochs = 87 and Loss = 0.368976\n","Epochs = 88 and Loss = 0.371496\n","Epochs = 89 and Loss = 0.370464\n","Epochs = 90 and Loss = 0.369785\n","Epochs = 91 and Loss = 0.357003\n","Epochs = 92 and Loss = 0.350326\n","Epochs = 93 and Loss = 0.335099\n","Epochs = 94 and Loss = 0.335187\n","Epochs = 95 and Loss = 0.334401\n","Epochs = 96 and Loss = 0.327909\n","Epochs = 97 and Loss = 0.313882\n","Epochs = 98 and Loss = 0.318103\n","Epochs = 99 and Loss = 0.295727\n","Epochs = 100 and Loss = 0.312617\n","Epochs = 101 and Loss = 0.297482\n","Epochs = 102 and Loss = 0.28176\n","Epochs = 103 and Loss = 0.266401\n","Epochs = 104 and Loss = 0.277009\n","Epochs = 105 and Loss = 0.265057\n","Epochs = 106 and Loss = 0.256818\n","Epochs = 107 and Loss = 0.238505\n","Epochs = 108 and Loss = 0.240404\n","Epochs = 109 and Loss = 0.234821\n","Epochs = 110 and Loss = 0.233096\n","Epochs = 111 and Loss = 0.226301\n","Epochs = 112 and Loss = 0.229347\n","Epochs = 113 and Loss = 0.225699\n","Epochs = 114 and Loss = 0.2213\n","Epochs = 115 and Loss = 0.222571\n","Epochs = 116 and Loss = 0.223057\n","Epochs = 117 and Loss = 0.223967\n","Epochs = 118 and Loss = 0.207489\n","Epochs = 119 and Loss = 0.203868\n","Epochs = 120 and Loss = 0.194347\n","Epochs = 121 and Loss = 0.196323\n","Epochs = 122 and Loss = 0.19689\n","Epochs = 123 and Loss = 0.205051\n","Epochs = 124 and Loss = 0.193968\n","Epochs = 125 and Loss = 0.179818\n","Epochs = 126 and Loss = 0.186834\n","Epochs = 127 and Loss = 0.190595\n","Epochs = 128 and Loss = 0.188859\n","Epochs = 129 and Loss = 0.167393\n","Epochs = 130 and Loss = 0.17474\n","Epochs = 131 and Loss = 0.175545\n","Epochs = 132 and Loss = 0.159722\n","Epochs = 133 and Loss = 0.152175\n","Epochs = 134 and Loss = 0.154454\n","Epochs = 135 and Loss = 0.158539\n","Epochs = 136 and Loss = 0.133731\n","Epochs = 137 and Loss = 0.150182\n","Epochs = 138 and Loss = 0.149938\n","Epochs = 139 and Loss = 0.156274\n","Epochs = 140 and Loss = 0.146903\n","Epochs = 141 and Loss = 0.130723\n","Epochs = 142 and Loss = 0.144697\n","Epochs = 143 and Loss = 0.144423\n","Epochs = 144 and Loss = 0.126152\n","Epochs = 145 and Loss = 0.134825\n","Epochs = 146 and Loss = 0.132033\n","Epochs = 147 and Loss = 0.141047\n","Epochs = 148 and Loss = 0.123092\n","Epochs = 149 and Loss = 0.123631\n","Epochs = 150 and Loss = 0.127705\n","Epochs = 151 and Loss = 0.122721\n","Epochs = 152 and Loss = 0.117135\n","Epochs = 153 and Loss = 0.114714\n","Epochs = 154 and Loss = 0.124317\n","Epochs = 155 and Loss = 0.130593\n","Epochs = 156 and Loss = 0.116031\n","Epochs = 157 and Loss = 0.121711\n","Epochs = 158 and Loss = 0.113012\n","Epochs = 159 and Loss = 0.128703\n","Epochs = 160 and Loss = 0.115901\n","Epochs = 161 and Loss = 0.109198\n","Epochs = 162 and Loss = 0.111266\n","Epochs = 163 and Loss = 0.100309\n","Epochs = 164 and Loss = 0.093007\n","Epochs = 165 and Loss = 0.094555\n","Epochs = 166 and Loss = 0.092486\n","Epochs = 167 and Loss = 0.101948\n","Epochs = 168 and Loss = 0.094043\n","Epochs = 169 and Loss = 0.104144\n","Epochs = 170 and Loss = 0.08725\n","Epochs = 171 and Loss = 0.101331\n","Epochs = 172 and Loss = 0.085594\n","Epochs = 173 and Loss = 0.097912\n","Epochs = 174 and Loss = 0.097188\n","Epochs = 175 and Loss = 0.091995\n","Epochs = 176 and Loss = 0.088041\n","Epochs = 177 and Loss = 0.090564\n","Epochs = 178 and Loss = 0.08971\n","Epochs = 179 and Loss = 0.08717\n","Epochs = 180 and Loss = 0.079376\n","Epochs = 181 and Loss = 0.086491\n","Epochs = 182 and Loss = 0.080755\n","Epochs = 183 and Loss = 0.077057\n","Epochs = 184 and Loss = 0.087407\n","Epochs = 185 and Loss = 0.075694\n","Epochs = 186 and Loss = 0.086582\n","Epochs = 187 and Loss = 0.079619\n","Epochs = 188 and Loss = 0.075904\n","Epochs = 189 and Loss = 0.12219\n","Epochs = 190 and Loss = 0.094059\n","Epochs = 191 and Loss = 0.095415\n","Epochs = 192 and Loss = 0.091175\n","Epochs = 193 and Loss = 0.080283\n","Epochs = 194 and Loss = 0.077361\n","Epochs = 195 and Loss = 0.069667\n","Epochs = 196 and Loss = 0.0869\n","Epochs = 197 and Loss = 0.073806\n","Epochs = 198 and Loss = 0.074205\n","Epochs = 199 and Loss = 0.070363\n","Epochs = 200 and Loss = 0.071805\n","Accuracy = 98.3% and Loss = 0.062988\n","Accuracy = 60.15% and Loss = 2.935188\n"],"name":"stdout"}]}]}